![](http://www.kdocs.cn/api/v3/office/copy/bzIrWHQwQUlrZHl0MjZ5Q0taZTRIUnpySjNVK2Q4SWFxRzZUUlRTZWFleS9mVktSeVk3SUNkc2NlcE54YnpkVW90eWcrWGszZG5VSlo1SlhIVVdCS1FKV1dJem5weU5GLzVpT1c2alV6UlA1RkFheUswMDZIZ1B4WXlsQlZtUlE5MWw3RklsVC9DT2tFeVdqL3gzK1Ztdm54ckpsVzJGdFQwTXF2N1Q3VkFERDBrUE5hR2s0TU5KdnhZUnNncGtmdkhsWWJXR2YzV2ZnQXExTjdNa2RvVXc2aDZWMGFiRWNialBvcUVZZnVneG5ZZ1lIeC9IV3BCa3NPRlUyL2dHRFNpQXJBM3U3ZEhJPQ==/attach/object/HX64KBIA3E?)


# 1. 摘要

名字：创享干货日记

说明：创造、收集、分享干货资源

本人所有资源汇总：[https://kdocs.cn/l/cdW9A9J8spT0](https://kdocs.cn/l/cdW9A9J8spT0 "https://kdocs.cn/l/cdW9A9J8spT0")

——————————

列举的都是我听过的研究方向，以下大部分内容只有个名字，只有很小部分了解一点点，之后会加入一些人工智能理论常识，是我自己的学习记录，也是给大家的参考。

更多研究方向可以参考paperswithcode。

# 2. 方法类

是让功能类实现的方法汇总，例如机器学习、模型压缩、强化学习等。

## 2.1. 机器学习

让机器自己学习的方法汇总

### 2.1.1. 强化学习

听说是个大坑，很难而且需要庞大的算力，这个如果没人带还是别搞比较好。

### 2.1.2. 神经网络

在人工智能领域神经网络是人工神经网络的简称，人类模仿大脑神经元构思出了人工神经元，人工神经元组成了网络层，例如全连接层、卷积层、激活函数层、归一化层（和正态分布的标准化一样）等。

![](http://www.kdocs.cn/api/v3/office/copy/bzIrWHQwQUlrZHl0MjZ5Q0taZTRIUnpySjNVK2Q4SWFxRzZUUlRTZWFleS9mVktSeVk3SUNkc2NlcE54YnpkVW90eWcrWGszZG5VSlo1SlhIVVdCS1FKV1dJem5weU5GLzVpT1c2alV6UlA1RkFheUswMDZIZ1B4WXlsQlZtUlE5MWw3RklsVC9DT2tFeVdqL3gzK1Ztdm54ckpsVzJGdFQwTXF2N1Q3VkFERDBrUE5hR2s0TU5KdnhZUnNncGtmdkhsWWJXR2YzV2ZnQXExTjdNa2RvVXc2aDZWMGFiRWNialBvcUVZZnVneG5ZZ1lIeC9IV3BCa3NPRlUyL2dHRFNpQXJBM3U3ZEhJPQ==/attach/object/L364KBIA4I?)

大脑神经元

![](http://www.kdocs.cn/api/v3/office/copy/bzIrWHQwQUlrZHl0MjZ5Q0taZTRIUnpySjNVK2Q4SWFxRzZUUlRTZWFleS9mVktSeVk3SUNkc2NlcE54YnpkVW90eWcrWGszZG5VSlo1SlhIVVdCS1FKV1dJem5weU5GLzVpT1c2alV6UlA1RkFheUswMDZIZ1B4WXlsQlZtUlE5MWw3RklsVC9DT2tFeVdqL3gzK1Ztdm54ckpsVzJGdFQwTXF2N1Q3VkFERDBrUE5hR2s0TU5KdnhZUnNncGtmdkhsWWJXR2YzV2ZnQXExTjdNa2RvVXc2aDZWMGFiRWNialBvcUVZZnVneG5ZZ1lIeC9IV3BCa3NPRlUyL2dHRFNpQXJBM3U3ZEhJPQ==/attach/object/NL64KBIAWQ?)

人工神经元

输入层、隐藏层、输出层：这里的层与深度学习的层不太一样，输入层、输出层是没有计算操作，只是单纯的告诉你这是输入、输出的数据，隐藏层是中间计算过后的各种结果形成的层，例如卷积之后的数据为一层、激活函数计算之后的数据为一层，在深度学习中的层是指激活函数、归一化、卷积本身。

![](http://www.kdocs.cn/api/v3/office/copy/bzIrWHQwQUlrZHl0MjZ5Q0taZTRIUnpySjNVK2Q4SWFxRzZUUlRTZWFleS9mVktSeVk3SUNkc2NlcE54YnpkVW90eWcrWGszZG5VSlo1SlhIVVdCS1FKV1dJem5weU5GLzVpT1c2alV6UlA1RkFheUswMDZIZ1B4WXlsQlZtUlE5MWw3RklsVC9DT2tFeVdqL3gzK1Ztdm54ckpsVzJGdFQwTXF2N1Q3VkFERDBrUE5hR2s0TU5KdnhZUnNncGtmdkhsWWJXR2YzV2ZnQXExTjdNa2RvVXc2aDZWMGFiRWNialBvcUVZZnVneG5ZZ1lIeC9IV3BCa3NPRlUyL2dHRFNpQXJBM3U3ZEhJPQ==/attach/object/O364KBIAIY?)

输入层、隐藏层、输出层

单层感知器（也叫感知机）：单层神经网络

多层感知器：双层神经网络

深度神经网络：三层及以上的神经网络，例如transformer、图神经网络、卷积神经网络等，具体多少层算深，没有统一说法。

#### 2.1.2.1. （神经网络训练过程说明）

训练（train）数据流(pipline)：假设100张图片，一次训练5张，训练6轮（epoch）。

。。batch size：5。

。。epoch：6，英语中epoch是比iter更大的时间单位。

。。iter：迭代，100/5=20次，也就是每个epoch需要20次iter。

。。一个循环：也就是训练一次数据需要走过的流程，数据增强-神经网络-损失函数-梯度优化。每次iter都是一个循环

batch size：是为了降低内存、显存提出的方法，内存有限，无法读取全部图片，所以将所有图片分成多个批次（batch）进行计算，减轻负担。

电脑内存、显卡显存：将计算机比作人类的大脑，计算1+1=2的时候，人类大脑需要先临时记住1+1，之后才会处理数据得到2，机器也一样，需要先临时记住之后计算，临时记住的硬件在手机端是缓存、在电脑端是内存、在显卡中是显存。

手机内存、电脑存储空间：人类可以长期记忆一些东西，机器长期记忆的硬件手机端是内存、电脑端是存储空间、显卡没有。

CPU、GPU：用来计算的机器硬件是CPU、GPU等，CPU是万能芯片，可以做各种事情，可以搬运数据、记录数据、计算数据、存储数据，GPU是专门用来计算数据的，计算速度比CPU快得多。

验证（val）数据流(pipline)：

。。一个循环：数据增强（可能没有）-神经网络

测试（test）数据流(pipline)：

。。一个循环：数据增强（可能没有）-神经网络

#### 2.1.2.2. 深度学习

让深度神经网络学习进而获取一些能力的方法总和。

##### 2.1.2.2.1. transformer

##### 2.1.2.2.2. 图神经网络（Graph Neural Network，简称GNN）

##### 2.1.2.2.3. 卷积神经网络（CNN，Convolutional Neural Network）

由卷积层组成的深度学习网络

卷积：由小波分析中的滤波器演化而来，滤波器和卷积都可以理解成加权（权重）求和。

权重：颜色可以转化成数据，例如人肉眼的三原色为红绿蓝，三种颜色的组合是的我们看到多彩的世界，0-1表示颜色的深浅，红0.1、绿0.2、蓝0.3就会产生一种颜色，显示屏的每个像素由红绿蓝三种发光点组成，数字可以控制发光的亮度进而形成不同颜色。将红绿蓝看成数字1、2、3，那么0.1、0.2、0.3就是权重，所以最终得到的数据是0.1×1+0.2×2+0.3×3=1.4就可以用来代表这个所生成的颜色，这是权重的一个功能，权重还可以通过设置成不同数值和规模提取图片的不同信息，滤波器的权重一般都是人为设置的，卷积的权重会通过神经网络自动更改权重，进而得到人们想要的东西。权重可以理解成对已有的确定的数字的掌控和改变。

![](http://www.kdocs.cn/api/v3/office/copy/bzIrWHQwQUlrZHl0MjZ5Q0taZTRIUnpySjNVK2Q4SWFxRzZUUlRTZWFleS9mVktSeVk3SUNkc2NlcE54YnpkVW90eWcrWGszZG5VSlo1SlhIVVdCS1FKV1dJem5weU5GLzVpT1c2alV6UlA1RkFheUswMDZIZ1B4WXlsQlZtUlE5MWw3RklsVC9DT2tFeVdqL3gzK1Ztdm54ckpsVzJGdFQwTXF2N1Q3VkFERDBrUE5hR2s0TU5KdnhZUnNncGtmdkhsWWJXR2YzV2ZnQXExTjdNa2RvVXc2aDZWMGFiRWNialBvcUVZZnVneG5ZZ1lIeC9IV3BCa3NPRlUyL2dHRFNpQXJBM3U3ZEhJPQ==/attach/object/UT64KBIAHY?)

卷积图解

### 2.1.3. 对抗学习

### 2.1.4. 监督学习

### 2.1.5. 无监督学习

### 2.1.6. 生成对抗网络

### 2.1.7. 半监督学习

### 2.1.8. 双层优化

### 2.1.9. 自动机器学习

#### 2.1.9.1. 自动数据清洗

#### 2.1.9.2. 自动特征工程

#### 2.1.9.3. 元学习

#### 2.1.9.4. 少（小）样本学习

### 2.1.10. 网络架构搜索（NAS）

#### 2.1.10.1. 超参数优化

### 2.1.11. 进化算法

## 2.2. 模型压缩

也叫模型加速、轻量化设计、TinyML、实时算法、小型模型等，只要能让模型速度更快、体积更小、精度更高，都可以看作是模型压缩的一部分。

### 2.2.1. 网络架构搜索（NAS）

### 2.2.2. 纯transformer方案的压缩移植

### 2.2.3. CNN+transformer融合的方案

### 2.2.4. 剪枝（Pruning）

### 2.2.5. 量化（Quantization）

### 2.2.6. 知识蒸馏（Knowledge Distillation）

### 2.2.7. 网络结构压缩

也叫网络结构的巧妙设计，包括层级、模块级、整体网络的改进

# 3. 功能类

就是让人工智能做一些事情，例如计算机视觉让机器拥有人类视觉和分析图片的智慧。

## 3.1. 计算机视觉

让计算机拥有人类得智慧和视觉

### 3.1.1. 图像分类

### 3.1.2. 目标检测

#### 3.1.2.1. 2D目标检测

##### 3.1.2.1.1. 实时2D目标检测

#### 3.1.2.2. 3D目标检测

##### 3.1.2.2.1. 点云目标检测

#### 3.1.2.3. 小目标检测

检测数据中比较小的目标，为了让小目标更加清晰，图片数据往往比较大，所以数据集比较大，对应训练需要的算力更多，花钱更多。

##### 3.1.2.3.1. 红外小目标检测

#### 3.1.2.4. 实时目标检测

#### 3.1.2.5. 红外线目标检测

##### 3.1.2.5.1. 红外小目标检测

#### 3.1.2.6. 视频目标检测

#### 3.1.2.7. 人脸检测（识别）

#### 3.1.2.8. 动作检测（识别）

### 3.1.3. 实例分割

### 3.1.4. 语义分割

### 3.1.5. 姿态估计

### 3.1.6. 目标追踪

### 3.1.7. 三维重建

### 3.1.8. slam

#### 3.1.8.1. 语义slam

## 3.2. 自然语言处理

让计算机拥有和人交流得能力和智慧

### 3.2.1. 文本分析

### 3.2.2. 文本挖掘

### 3.2.3. 信息检索

### 3.2.4. 语言模型